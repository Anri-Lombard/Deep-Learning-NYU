# NYU Deep Learning Course by Yann LeCun

## Course Structure
__Week 1__
- History and Motivation
- Evolution and DL
- Neural Nets
__Week 2__
- SGD and backpropagation
- Backprop in practice
- NN training
__Week 3__
- Parameter transformation
- Convolutional Nets
- Natural signals' properties
__Week 4__
- 1 Dimentionsal Convolutional Nets
__Week 5__
- Optimization
- Autograd
- CNNs (again)
__Week 6__
- CNN applications
- Recurrent Nets and attention
- Training Recurrent Nets
__Week 7__
- Energy-based models
- Self-supervised learning (SSL), Explainable Boosting Machines (EBM)
- Autoencoders
__Week 8__
- Contrastive methods
- Regularised latent
- Training Variable Autoencoders
__Week 9__
- Sparsity
- World models, Generative Adversarial Networks (GANs)
- Training GANs
__Week 10__
- CV SSL
- Predictive Control
__Week 11__
- Activations
- Losses
- PPUU
__Week 12__
- Deep Learning for Natural Language Processing (NLP)
- Attention and transformers
__Week 13__
- Graph Convolutional Networks (GCNs)
__Week 14__
- Structured prediction
- Graphical Methods
- Regularization and Bayesian methods
__Week 15__
- Interface for latent-variable EBMs
- Training latent-variable EBMs

__IN PROGRESS...__

## References
[NYU Course](https://atcold.github.io/pytorch-Deep-Learning/)